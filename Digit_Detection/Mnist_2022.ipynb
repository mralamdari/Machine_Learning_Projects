{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdSsxaXfmRpps8ftmfuaKl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Machine_Learning_Projects/blob/main/Mnist_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNOtI50P0i7X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size=784\n",
        "hidden_size=400   #hidden_size= (input_size + out_size) // 2\n",
        "out_size=10\n",
        "epochs=10\n",
        "batch_size=100\n",
        "learning_rate=0.001"
      ],
      "metadata": {
        "id": "I6KmilZrFCH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_gray = 0.1307\n",
        "std_grey = 0.3081\n",
        "\n",
        "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                                             torchvision.transforms.Normalize((mean_gray, ), (std_grey, ))])"
      ],
      "metadata": {
        "id": "tVhYCS6GGJfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = torchvision.datasets.MNIST(root='/data',\n",
        "                           train = True,\n",
        "                           transform = torchvision.transforms.ToTensor(),\n",
        "                           download=True)\n",
        "\n",
        "test_ds = torchvision.datasets.MNIST(root='/data',\n",
        "                                  train=False,\n",
        "                                  transform = torchvision.transforms.ToTensor(),\n",
        "                                  download=True)"
      ],
      "metadata": {
        "id": "JDFdMsCA0q4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_ds,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_ds, \n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "33nOBQVA1sn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
        "    self.batchnorm1 = nn.BatchNorm2d(8)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "    self.batchnorm2 = nn.BatchNorm2d(32)\n",
        "    self.fc1 = nn.Linear(32*7*7, 600)\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "    self.fc2 = nn.Linear(600, 100)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.batchnorm1(out)\n",
        "    out = self.relu(out)  \n",
        "    out = self.maxpool(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.batchnorm2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.maxpool(out)\n",
        "\n",
        "    out = out.view(-1, 1568)\n",
        "\n",
        "    out = self.fc1(out)\n",
        "    out = self.dropout(out)\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "4Im3OMgl22V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "_lfqPRpEJ-G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=20\n",
        "train_loss = []\n",
        "train_accuracy = []\n",
        "test_loss = []\n",
        "test_accuracy = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  correct = 0\n",
        "  iterations = 0\n",
        "  iter_loss = 0.0\n",
        "\n",
        "  model.train()\n",
        "  for i, (inputs, labels) in enumerate(train_loader):\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    iter_loss += loss.item()\n",
        "    optimizer.zero_grad() \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, pred = torch.max(outputs, 1)\n",
        "    correct += (pred == labels).sum().item()\n",
        "    iterations += 1\n",
        "  \n",
        "  train_loss.append(iter_loss/iterations)\n",
        "  train_accuracy.append(100 * correct/ len(train_ds))\n",
        "  \n",
        "  t_loss = 0.0\n",
        "  correct = 0\n",
        "  iterations = 0\n",
        "  model.eval()\n",
        "  for i, (inputs, labels) in enumerate(test_loader):\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    t_loss += loss.item()\n",
        "    _, pred = torch.max(outputs, 1) #(100, 10)  ==> 10 is in index 1\n",
        "    correct += (pred == labels).sum().item()\n",
        "    iterations += 1\n",
        "\n",
        "    test_loss.append(t_loss/iterations)\n",
        "    test_accuracy.append(100*correct/len(test_ds))\n",
        "\n",
        "\n",
        "  print(f'epoch: {epoch}, Training Loss: {train_loss[-1]:.3f}, Training Accuracy: %{train_accuracy[-1]:.3f}, Test Loss: {test_loss[-1]:.3f}, Test Accuracy:% {test_accuracy[-1]:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z1hn8IMPqfB",
        "outputId": "745bf7c5-1342-4b23-a87f-dc14036110d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, Training Loss: 0.119, Training Accuracy: %97.508, Test Loss: 0.131, Test Accuracy:% 97.760\n",
            "epoch: 1, Training Loss: 0.110, Training Accuracy: %97.672, Test Loss: 0.106, Test Accuracy:% 97.330\n",
            "epoch: 2, Training Loss: 0.109, Training Accuracy: %97.673, Test Loss: 0.086, Test Accuracy:% 98.390\n",
            "epoch: 3, Training Loss: 0.115, Training Accuracy: %97.658, Test Loss: 0.145, Test Accuracy:% 97.490\n",
            "epoch: 4, Training Loss: 0.121, Training Accuracy: %97.523, Test Loss: 0.110, Test Accuracy:% 97.910\n",
            "epoch: 5, Training Loss: 0.102, Training Accuracy: %97.762, Test Loss: 0.114, Test Accuracy:% 97.890\n",
            "epoch: 6, Training Loss: 0.112, Training Accuracy: %97.652, Test Loss: 0.096, Test Accuracy:% 97.720\n",
            "epoch: 7, Training Loss: 0.115, Training Accuracy: %97.557, Test Loss: 0.095, Test Accuracy:% 97.620\n",
            "epoch: 8, Training Loss: 0.121, Training Accuracy: %97.507, Test Loss: 0.109, Test Accuracy:% 98.030\n",
            "epoch: 9, Training Loss: 0.117, Training Accuracy: %97.625, Test Loss: 0.144, Test Accuracy:% 97.870\n",
            "epoch: 10, Training Loss: 0.107, Training Accuracy: %97.747, Test Loss: 0.125, Test Accuracy:% 97.770\n",
            "epoch: 11, Training Loss: 0.135, Training Accuracy: %97.302, Test Loss: 0.117, Test Accuracy:% 97.370\n",
            "epoch: 12, Training Loss: 0.123, Training Accuracy: %97.372, Test Loss: 0.109, Test Accuracy:% 97.580\n",
            "epoch: 13, Training Loss: 0.103, Training Accuracy: %97.597, Test Loss: 0.100, Test Accuracy:% 98.070\n",
            "epoch: 14, Training Loss: 0.108, Training Accuracy: %97.537, Test Loss: 0.096, Test Accuracy:% 98.280\n",
            "epoch: 15, Training Loss: 0.103, Training Accuracy: %97.563, Test Loss: 0.112, Test Accuracy:% 97.850\n",
            "epoch: 16, Training Loss: 0.109, Training Accuracy: %97.575, Test Loss: 0.100, Test Accuracy:% 97.960\n",
            "epoch: 17, Training Loss: 0.122, Training Accuracy: %97.410, Test Loss: 0.099, Test Accuracy:% 98.510\n",
            "epoch: 18, Training Loss: 0.121, Training Accuracy: %97.428, Test Loss: 0.101, Test Accuracy:% 98.060\n",
            "epoch: 19, Training Loss: 0.105, Training Accuracy: %97.663, Test Loss: 0.096, Test Accuracy:% 97.740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "def standardization(x):\n",
        "    return (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n",
        "\n",
        "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, random_state=77, test_size=0.2)\n",
        "x_train_temp = standardization(x_train)\n",
        "x_test_temp = standardization(x_test)\n",
        "x_train_std, x_test_std = np.nan_to_num(x_train_temp), np.nan_to_num(x_test_temp)\n",
        "\n",
        "mnist = sklearn.datasets.fetch_openml('mnist_784', version=1)\n",
        "x, y = mnist.data, mnist.target\n",
        "clf = sklearn.ensemble.RandomForestClassifier()\n",
        "clf.fit(x_train, y_train)\n",
        "sgd_pred = clf.predict(x_test)\n",
        "sklearn.metrics.accuracy_score(y_test, sgd_pred)"
      ],
      "metadata": {
        "id": "eCMDBnNZkIYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(32)\n",
        "\n",
        "train_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)\n",
        "test_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)\n",
        "augmented_datgen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255,\n",
        "                                                                 width_shift_range=0.2, \n",
        "                                                                 height_shift_range=0.2,\n",
        "                                                                 shear_range=0.2,\n",
        "                                                                 rotation_range=25,\n",
        "                                                                 zoom_range=0.234)\n",
        "\n",
        "train_dir = \"/content/pizza_steak/train\"\n",
        "test_dir = \"/content/pizza_steak/test\"\n",
        "\n",
        "\n",
        "train_data = train_gen.flow_from_directory(train_dir,\n",
        "                                               batch_size=32, # number of images to process at a time \n",
        "                                               target_size=(224, 224), # convert all images to be 224 x 224\n",
        "                                               class_mode=\"binary\", # type of problem we're working on\n",
        "                                               seed=42)\n",
        "\n",
        "valid_data = test_gen.flow_from_directory(test_dir,\n",
        "                                               batch_size=32,\n",
        "                                               target_size=(224, 224),\n",
        "                                               class_mode=\"binary\",\n",
        "                                               seed=42)\n",
        "\n",
        "augmented_train = augmented_datgen.flow_from_directory(directory=train_dir, batch_size=32, target_size=(224, 224), class_mode=\"binary\", seed=32)"
      ],
      "metadata": {
        "id": "-H_XNLBaP-TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(filters=10, \n",
        "                         kernel_size=3, # can also be (3, 3)\n",
        "                         activation=\"relu\", \n",
        "                         input_shape=(224, 224, 3)), # first layer specifies input shape (height, width, colour channels)\n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n",
        "  tf.keras.layers.MaxPool2D(pool_size=2, # pool_size can also be (2, 2)\n",
        "                            padding=\"valid\"), # padding can also be 'same'\n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"), # activation='relu' == tf.keras.layers.Activations(tf.nn.relu)\n",
        "  tf.keras.layers.MaxPool2D(2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(1, activation=\"sigmoid\") # binary activation output\n",
        "])\n",
        "# Compile the model\n",
        "model_1.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "# Fit the model\n",
        "history_1 = model_1.fit(train_data,\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=len(train_data),\n",
        "                        validation_data=valid_data,\n",
        "                        validation_steps=len(valid_data))"
      ],
      "metadata": {
        "id": "k_v7oMvjl6FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
        "x = tf.keras.layers.Conv2D(128, 3, 1, activation=\"relu\")(inputs)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "x = tf.keras.layers.Conv2D(128, 3, 1, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "# x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model_1.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                metrics=\"accuracy\")\n",
        "\n",
        "model_1_history=model_1.fit(x_train.reshape((-1 ,28, 28)), y_train, epochs=10, validation_data=(x_test.reshape((-1 ,28, 28)), y_test))\n",
        "preds = model_1.predict(x_test.reshape(-1, 28, 28, 1))\n",
        "model_1.evaluate(x_train.reshape(-1, 28, 28, 1), y_train), model_1.evaluate(x_test.reshape(-1, 28, 28, 1), y_test)"
      ],
      "metadata": {
        "id": "JFjgK1yWl6BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ctYamgBmEQu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
