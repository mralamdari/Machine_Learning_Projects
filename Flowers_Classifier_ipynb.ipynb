{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEekbGyaSpvAkm7aIEgElk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Machine_Learning_Projects/blob/main/Flowers_Classifier_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hello üôå\n",
        "In this Notebook, A Classifier is implemented in TensorFlow to classify images of [Flowers dataset](https://www.tensorflow.org/datasets/catalog/tf_flowers) from tensorflow, It contains 3670 images falling into 5 categories:\n",
        "\n",
        "\n",
        "*   Daisy üíê\n",
        "*   Roses üåπ\n",
        "*   Dandelion üíÆ\n",
        "*   SunFlower üåª\n",
        "*   Tulip üå∑\n",
        "\n",
        "you can access it from here:\n",
        "\n",
        "```gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/```\n",
        "\n"
      ],
      "metadata": {
        "id": "6Ohxs5wsLjS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Neccessary Libraries üêæ"
      ],
      "metadata": {
        "id": "zc3bL53sOi9H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Tmw9hJYxLG4Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.version.VERSION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_Mj5o_FN21P",
        "outputId": "784a64a5-5930-45b3-d0f7-3d4a17bed038"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_name = tf.test.gpu_device_name()\n",
        "print(f'The GPU found at: {gpu_name}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0vOsX8mN8zS",
        "outputId": "2013b597-e3b1-4559-87c5-08361b247170"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The GPU found at: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Data ‚öì"
      ],
      "metadata": {
        "id": "Yg7-0Lg-Ooj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/'\n",
        "\n",
        "!gsutil cat gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/train_set.csv | head -50"
      ],
      "metadata": {
        "id": "B1tMwvlGODaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95895894-b4e5-4421-91ed-2836d23ec6df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/754296579_30a9ae018c_n.jpg,daisy\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/18089878729_907ed2c7cd_m.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/284497199_93a01f48f6.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/3554992110_81d8c9b0bd_m.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/4065883015_4bb6010cb7_n.jpg,daisy\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/roses/7420699022_60fa574524_m.jpg,roses\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/4558536575_d43a611bd4_n.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/7568630428_8cf0fc16ff_n.jpg,daisy\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/tulips/7064813645_f7f48fb527.jpg,tulips\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/sunflowers/4933229095_f7e4218b28.jpg,sunflowers\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/14523675369_97c31d0b5b.jpg,daisy\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/sunflowers/21518663809_3d69f5b995_n.jpg,sunflowers\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/15782158700_3b9bf7d33e_m.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/tulips/8713398906_28e59a225a_n.jpg,tulips\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/tulips/6770436217_281da51e49_n.jpg,tulips\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/8754822932_948afc7cef.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/22873310415_3a5674ec10_m.jpg,daisy\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/sunflowers/5967283168_90dd4daf28_n.jpg,sunflowers\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/roses/483444865_65962cea07_m.jpg,roses\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/2229906591_e953785d13.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/2520369272_1dcdb5a892_m.jpg,daisy\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/tulips/6903831250_a2757fff82_m.jpg,tulips\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/roses/22679076_bdb4c24401_m.jpg,roses\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/7227973870_806d9d3e42_n.jpg,daisy\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/roses/6016195304_75306bb79a.jpg,roses\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/roses/18389368680_91c24a2087_z.jpg,roses\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/3991962484_085ba2da94.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/sunflowers/4932736308_827012cff2.jpg,sunflowers\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/sunflowers/4933230247_a0432f01da.jpg,sunflowers\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/roses/8050213579_48e1e7109f.jpg,roses\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/4500964841_b1142b50fb_n.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/2838487505_6c3b48efa5_m.jpg,daisy\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/sunflowers/877083343_e3338c4125.jpg,sunflowers\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/7808545612_546cfca610_m.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/19440660848_c789227129_m.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/2540640433_dedd577263.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/129019877_8eea2978ca_m.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/18203367608_07a04e98a4_n.jpg,daisy\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/7132677385_bcbdcc6001.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/roses/15202632426_d88efb321a_n.jpg,roses\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/tulips/13542672763_20c3cb9272.jpg,tulips\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/sunflowers/5020805619_6c710793f7.jpg,sunflowers\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/sunflowers/20704967595_a9c9b8d431.jpg,sunflowers\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/roses/9609569441_eeb8566e94.jpg,roses\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/10712722853_5632165b04.jpg,daisy\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dandelion/2479491210_98e41c4e7d_m.jpg,dandelion\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/tulips/2481827798_6087d71134.jpg,tulips\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/18684594849_7dd3634f5e_n.jpg,daisy\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/13977181862_f8237b6b52.jpg,daisy\n",
            "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/sunflowers/22686342422_c0b9e2f38e.jpg,sunflowers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As tou can see, data \"\"TRAIN_SET\"\" has a path to each Image and It's type as well.\n",
        "\n",
        "For Example:\n",
        "\n",
        "```\n",
        "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/10712722853_5632165b04.jpg,daisy\n",
        "```\n",
        "\n",
        "Has the Format:\n",
        "\n",
        "\n",
        "```\n",
        "PATH, Flower_Type\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cRp16tnsOwvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading the Images üìñ"
      ],
      "metadata": {
        "id": "8o2L-nRHPvc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Image_read_decoder(filename, reshape_dims=(224, 224)):\n",
        "  img = tf.io.read_file(filename) #Read the Image as a compressed String.\n",
        "  img = tf.image.decode_jpeg(img, channels=3) #Decode The Compressed String to a 3-D unit8 Tensor.\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32) #Normalize the Tensors in to [0, 1] Range\n",
        "  img = tf.image.resize(img, reshape_dims)\n",
        "  return img"
      ],
      "metadata": {
        "id": "YpRShaaTOth3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_img = f\"{path}dandelion/18089878729_907ed2c7cd_m.jpg\"\n",
        "img_H, img_W, img_C = 224, 224, 3\n",
        "Image_read_decoder(sample_img, (224, 224))"
      ],
      "metadata": {
        "id": "4JB9feihRVKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f57412-66e2-4c3e-98a8-f157aaf4f7f3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(224, 224, 3), dtype=float32, numpy=\n",
              "array([[[0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
              "        ...,\n",
              "        [1.1648739e-03, 1.1648739e-03, 1.1648739e-03],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n",
              "\n",
              "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
              "        ...,\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n",
              "\n",
              "       [[7.0027937e-04, 7.0027937e-04, 7.0027937e-04],\n",
              "        [2.9074095e-04, 2.9074095e-04, 2.9074095e-04],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
              "        ...,\n",
              "        [4.3143428e-04, 4.3143428e-04, 4.3143428e-04],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[2.8011547e-03, 2.8011547e-03, 2.8011547e-03],\n",
              "        [1.1629793e-03, 1.1629793e-03, 1.1629793e-03],\n",
              "        [3.4082348e-03, 3.4082348e-03, 3.4082348e-03],\n",
              "        ...,\n",
              "        [2.5399467e-01, 2.5399467e-01, 2.5399467e-01],\n",
              "        [2.3920615e-01, 2.3920615e-01, 2.3920615e-01],\n",
              "        [2.2913149e-01, 2.2913149e-01, 2.2913149e-01]],\n",
              "\n",
              "       [[1.4005534e-02, 1.4005534e-02, 1.4005534e-02],\n",
              "        [1.1957850e-02, 1.1957850e-02, 1.1957850e-02],\n",
              "        [3.4620568e-02, 3.4620568e-02, 3.4620568e-02],\n",
              "        ...,\n",
              "        [2.9907840e-01, 2.9907840e-01, 2.9907840e-01],\n",
              "        [2.8781626e-01, 2.8781626e-01, 2.8781626e-01],\n",
              "        [2.7815109e-01, 2.7815109e-01, 2.7815109e-01]],\n",
              "\n",
              "       [[1.2913090e-01, 1.2913090e-01, 1.2913090e-01],\n",
              "        [1.4674091e-01, 1.4674091e-01, 1.4674091e-01],\n",
              "        [1.3223600e-01, 1.3223600e-01, 1.3223600e-01],\n",
              "        ...,\n",
              "        [2.0302311e-01, 2.0302311e-01, 2.0302311e-01],\n",
              "        [2.3066829e-01, 2.3066829e-01, 2.3066829e-01],\n",
              "        [2.3697512e-01, 2.3697512e-01, 2.3697512e-01]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_paths = tf.io.gfile.glob(f'{path}*')\n",
        "path_names = tf.strings.regex_replace(all_paths, path, \"\")\n",
        "class_names = list()\n",
        "for p in path_names:\n",
        "  decoded_name = p.numpy().decode('utf-8')\n",
        "  if decoded_name.find('.') < 0:\n",
        "    class_names.append(decoded_name)\n",
        "print(class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eLj8fCSVEG1",
        "outputId": "d442dcec-2622-4c8b-82b9-746721bfd33b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### So All the images are in 5 categories:\n",
        "\n",
        "\n",
        "```\n",
        "'daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7giAnPolnYLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Visualization üëÄ"
      ],
      "metadata": {
        "id": "sI7HXXkGd_Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_img(img_path):\n",
        "  img = Image_read_decoder(img_path)\n",
        "  plt.imshow(img.numpy())"
      ],
      "metadata": {
        "id": "HlXZFEYNVfDk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_img(sample_img)"
      ],
      "metadata": {
        "id": "ptA3jm5ie-88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did you enjoy it? üôÇ let's Visualize more images ü•≥ ü•≥ ü•≥"
      ],
      "metadata": {
        "id": "AybcrDbOflCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at These Lovely Daisies üíê"
      ],
      "metadata": {
        "id": "rs7lWwtInQem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 5, figsize=(10, 10))\n",
        "files_path = tf.io.gfile.glob(f'{path}{class_names[0]}/*.jpg')[:5]\n",
        "for ind, img_path in enumerate(files_path):\n",
        "    img = Image_read_decoder(img_path)\n",
        "    ax[ind].imshow((img.numpy()))\n",
        "    ax[ind].axis('off')"
      ],
      "metadata": {
        "id": "haj84KrhfIaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you love dandelions? üíÆ"
      ],
      "metadata": {
        "id": "sauvx0_tntmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 5, figsize=(10, 10))\n",
        "files_path = tf.io.gfile.glob(f'{path}{class_names[1]}/*.jpg')[:5]\n",
        "for ind, img_path in enumerate(files_path):\n",
        "    img = Image_read_decoder(img_path)\n",
        "    ax[ind].imshow((img.numpy()))\n",
        "    ax[ind].axis('off')"
      ],
      "metadata": {
        "id": "3fKKiYnCkXsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I know The Roses üåπ are the most favorite ones"
      ],
      "metadata": {
        "id": "-Kd34oJIoPE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 5, figsize=(10, 10))\n",
        "files_path = tf.io.gfile.glob(f'{path}{class_names[2]}/*.jpg')[:5]\n",
        "for ind, img_path in enumerate(files_path):\n",
        "    img = Image_read_decoder(img_path)\n",
        "    ax[ind].imshow((img.numpy()))\n",
        "    ax[ind].axis('off')"
      ],
      "metadata": {
        "id": "ZheuSgB5nD7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you Love SunFlower üåª for Itself or It's achene?"
      ],
      "metadata": {
        "id": "lXQBrclQolyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 5, figsize=(10, 10))\n",
        "files_path = tf.io.gfile.glob(f'{path}{class_names[3]}/*.jpg')[:5]\n",
        "for ind, img_path in enumerate(files_path):\n",
        "    img = Image_read_decoder(img_path)\n",
        "    ax[ind].imshow((img.numpy()))\n",
        "    ax[ind].axis('off')"
      ],
      "metadata": {
        "id": "tR4D4DM4nFzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tulips üå∑ are my favorites"
      ],
      "metadata": {
        "id": "Nz7rQRkKo-SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 5, figsize=(10, 10))\n",
        "files_path = tf.io.gfile.glob(f'{path}{class_names[4]}/*.jpg')[:5]\n",
        "for ind, img_path in enumerate(files_path):\n",
        "    img = Image_read_decoder(img_path)\n",
        "    ax[ind].imshow((img.numpy()))\n",
        "    ax[ind].axis('off')"
      ],
      "metadata": {
        "id": "DwYIbsYInKC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_csv(csv_row, H=224, W=224):\n",
        "  record_defaults = ['path', 'flower']\n",
        "  filename, label_string = tf.io.decode_csv(csv_row, record_defaults)\n",
        "  img = Image_read_decoder(filename, [H, W])\n",
        "  label = tf.argmax(tf.math.equal(class_names, label_string))\n",
        "  return img, label"
      ],
      "metadata": {
        "id": "n2Ysu_b3fvPq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_path = \"gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/train_set.csv\"\n",
        "\n",
        "eval_set_path = \"gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/eval_set.csv\"\n",
        "\n",
        "train_dataset = (tf.data.TextLineDataset(train_set_path).map(decode_csv)).batch(32)\n",
        "\n",
        "eval_dataset = (tf.data.TextLineDataset(eval_set_path).map(decode_csv)).batch(32)"
      ],
      "metadata": {
        "id": "cgxD7Q1FpEA-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regularizer = tf.keras.regularizers.l1_l2(0.5, 0.5)"
      ],
      "metadata": {
        "id": "V5jG16ToCiAj"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(img_H, img_W, img_C)),\n",
        "    tf.keras.layers.Dense(100, activation='relu',kernel_regularizer=regularizer),\n",
        "    tf.keras.layers.Dense(len(class_names), activation='softmax', kernel_regularizer=regularizer)])"
      ],
      "metadata": {
        "id": "FKe_yHmliHpl"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "C3r16DeJkZ2l"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXMlf-1OlfBF",
        "outputId": "ea3a9f50-950f-4439-8cdc-68d8b85072ce"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_2 (Flatten)         (None, 150528)            0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 100)               15052900  \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 5)                 505       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,053,405\n",
            "Trainable params: 15,053,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=False)"
      ],
      "metadata": {
        "id": "AuChpSsBljEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset, validation_data=eval_dataset, epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP4OJ0E9mCR_",
        "outputId": "9cd69af7-a68c-409a-cfea-1e3eddf3c5b8"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "330/330 [==============================] - 166s 500ms/step - loss: 1260.7294 - accuracy: 0.2306 - val_loss: 809.7908 - val_accuracy: 0.2351\n",
            "Epoch 2/2\n",
            "330/330 [==============================] - 147s 446ms/step - loss: 795.7261 - accuracy: 0.2458 - val_loss: 789.4197 - val_accuracy: 0.2351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_plot(history):\n",
        "  metrics = ['loss', 'accuracy']\n",
        "  f, ax = plt.subplots(1, len(metrics), figsize=(15, 5))\n",
        "  for idx, metric in enumerate(metrics):\n",
        "    ax[idx].plot(history.history[metric])\n",
        "    ax[idx].set_xlabel('Epochs')\n",
        "    ax[idx].set_ylabel(metric, fontweight='bold', fontsize=20)\n",
        "    ax[idx].plot(history.history[f'val_{metric}'], ls='dashed')\n",
        "    ax[idx].legend([metric, f'val_{metric}'], fontsize=20)"
      ],
      "metadata": {
        "id": "k9SMz0dLnG9i"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_plot(history)"
      ],
      "metadata": {
        "id": "mm6lhpv-rr4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trained_weights_ilustrater(model, LAYER=1, WEIGHT_TYPE=0):\n",
        "  f, ax = plt.subplots(1, 5, figsize=(15,15))\n",
        "  for flower_ind in range(len(class_names)):\n",
        "    weights = model.layers[LAYER].get_weights()[WEIGHT_TYPE][:, flower_ind]\n",
        "    min_weight = tf.math.reduce_min(weights).numpy()\n",
        "    max_weight = tf.math.reduce_max(weights).numpy()\n",
        "    flower_name = class_names[flower_ind]\n",
        "    print(f\"Scaling weights for {flower_name} in {min_weight} to {max_weight}\")\n",
        "    weights = (weights - min_weight)/(max_weight - min_weight)\n",
        "    ax[flower_ind].imshow(weights.reshape(img_H, img_W, 3));\n",
        "    ax[flower_ind].set_title(flower_name);"
      ],
      "metadata": {
        "id": "EUnhtOnVsGhD"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_weights_ilustrater(model, 1)"
      ],
      "metadata": {
        "id": "yeOGqHWBusHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning Hyperparameters"
      ],
      "metadata": {
        "id": "J0F83luUbb6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CulFVIdZbdY8",
        "outputId": "fa91256d-9c71-468a-9153-74940334c3c3"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 135 kB 11.8 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.6 MB 50.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "def model_creater(hp):\n",
        "  lrate = hp.Float('lrate', 1e-4, 1e-1, sampling='log')\n",
        "  l1 = 0\n",
        "  l2 = hp.Choice('l2', values=[0.0, 1e-1, 1e-2, 1e-3, 1e-4])\n",
        "  num_hidden = hp.Int('num_hidden', 32, 256, 32)\n",
        "\n",
        "  regularizer = tf.keras.regularizers.l1_l2(l1, l2)\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(img_H, img_W, img_C)),\n",
        "    tf.keras.layers.Dense(num_hidden, activation='relu',kernel_regularizer=regularizer),\n",
        "    tf.keras.layers.Dense(len(class_names), activation='softmax', kernel_regularizer=regularizer)])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lrate),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "CPKcbpHCbdyN"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.BayesianOptimization(model_creater,\n",
        "                                objective = kt.Objective('val_accuracy', 'max'),\n",
        "                                max_trials=10,\n",
        "                                num_initial_points=2,\n",
        "                                overwrite=False)\n",
        "tuner.search(train_dataset,\n",
        "             validation_data=eval_dataset,\n",
        "             epochs=5,\n",
        "             callbacks=[tf.keras.callbacks.EarlyStopping(patience=1)])"
      ],
      "metadata": {
        "id": "5neiKEjgeMBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topN = 1\n",
        "for x in range(topN):\n",
        "  print(tuner.get_best_hyperparameters(topN)[x].values)\n",
        "  print(tuner.get_best_models(topN)[x].summary())"
      ],
      "metadata": {
        "id": "T0Ii8oZ8fdfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_hidden = [64, 16]\n",
        "layers = [tf.keras.layers.Flatten(input_shape=(img_H, img_W, img_C), name='input_pixels')]\n",
        "\n",
        "for hno, nodes in enumerate(num_hidden):\n",
        "  layers.extend([\n",
        "    tf.keras.layers.Dense(nodes, kernel_regularizer=regularizer, name=f'hidden_dense_{hno}'),\n",
        "    tf.keras.layers.BatchNormalization(scale=False, # ReLU\n",
        "                                        center=False, # have bias in Dense\n",
        "                                        name=f'batchnorm_dense_{hno}'),\n",
        "    #move activation to come after batchnorm\n",
        "    tf.keras.layers.Activation('relu', name=f'relu_dense_{hno}'),\n",
        "    tf.keras.layers.Dropout(rate=dropout_prob,\n",
        "                            name=f'dropout_dense_{hno}'))]\n",
        "\n",
        "layers.append(tf.keras.layers.Dense(len(class_names), \n",
        "                            kernel_regularizer=regularizer,\n",
        "                            activation='softmax',\n",
        "                            name='flower_prob'))"
      ],
      "metadata": {
        "id": "F-cHgdCmgBl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d9e3WSrSgv_k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}